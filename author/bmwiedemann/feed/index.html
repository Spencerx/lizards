<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>bmwiedemann &#8211; openSUSE Lizards</title>
	<atom:link href="https://lizards.opensuse.org/author/bmwiedemann/feed/" rel="self" type="application/rss+xml" />
	<link>https://lizards.opensuse.org</link>
	<description>Blogs and Ramblings of the openSUSE Members</description>
	<lastBuildDate>Fri, 06 Mar 2020 11:29:40 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.7.5</generator>
	<item>
		<title>openSUSE on reproducible builds summit</title>
		<link>https://lizards.opensuse.org/2019/12/13/opensuse-on-reproducible-builds-summit/</link>
		<pubDate>Fri, 13 Dec 2019 11:43:37 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Build Service]]></category>
		<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[reproducible-builds]]></category>
		<category><![CDATA[Summit]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=14231</guid>
		<description><![CDATA[As in the past 3 years, I joined the r-b summit where many people interested in reproducible builds met. There were several participants from companies, including Microsoft, Huawei and Google. Also some researchers from universities that work on tools like DetTrace, tuf and in-toto. But the majority still came from various open-source projects &#8211; with [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>As in the past 3 years, I joined <a href="https://reproducible-builds.org/events/Marrakesh2019/">the r-b summit</a> where many people interested in reproducible builds met.</p>
<p>There were several participants from companies, including Microsoft, Huawei and Google.<br />
Also some researchers from universities that work on tools like <a href="http://nickrenner.us-east-2.elasticbeanstalk.com/projects/dettrace">DetTrace</a>, tuf and in-toto.<br />
But the majority still came from various open-source projects &#8211; with Fedora/RedHat being notably absent.</p>
<p>We had many good discussion rounds, one of which spawned my writeup on <a href="https://lists.reproducible-builds.org/pipermail/rb-general/2019-December/001732.html">the goal of reproducible builds</a></p>
<p>Another session was about our wish to design a nice interface, where people can easily find the reproducibility status of a package in various distributions. I might code a Proof-of-Concept of that in the next weeks (when I have time).<br />
I also got some help with java patches in openSUSE and made several nice upstream reproducibility fixes &#8211; showing some others how easy that can be.</p>
<p>This whole event also was good team-building, getting to know each other better. This will allow us to better collaborate in the Future.</p>
<p>Later there will be a larger report compiled by others.</p>
]]></content:encoded>
			</item>
		<item>
		<title>openSUSE OBS git mirror</title>
		<link>https://lizards.opensuse.org/2019/09/13/opensuse-obs-git-mirror/</link>
		<comments>https://lizards.opensuse.org/2019/09/13/opensuse-obs-git-mirror/#comments</comments>
		<pubDate>Fri, 13 Sep 2019 19:42:12 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Build Service]]></category>
		<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Factory]]></category>
		<category><![CDATA[Miscellaneous]]></category>
		<category><![CDATA[Packaging]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[git]]></category>
		<category><![CDATA[IPFS]]></category>
		<category><![CDATA[mirror]]></category>
		<category><![CDATA[obs]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=14032</guid>
		<description><![CDATA[There was some discussion about our OBS and how in contrast Gentoo, VoidLinux or Fedora used git to track packages. So I made an experimental openSUSE:Factory git mirror to see how well it goes and how using it feels. The repo currently needs around 1GB but will slowly grow over time. I did not want [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>There was some discussion about our OBS and how in contrast <a href="https://github.com/gentoo/gentoo">Gentoo</a>, <a href="https://github.com/void-linux/void-packages/tree/master/srcpkgs/aubio/">VoidLinux</a> or <a href="https://src.fedoraproject.org/rpms/aubio/tree/master">Fedora</a> used git to track packages.</p>
<p>So I made an experimental <a href="https://github.com/bmwiedemann/openSUSE/">openSUSE:Factory git mirror</a> to see how well it goes and how using it feels.<br />
The repo currently needs around 1GB but will slowly grow over time. I did not want to spend effort to import all history.</p>
<p>Binary files are replaced by cryptographically secure symlinks into IPFS<br />
and I am currently providing files up to 9MB there.</p>
<p>If you can not run ipfs, you can still get these files through any of the public gateways like this:<br />
<code>curl https://ipfs.io$(readlink packages/a/aubio/aubio-0.4.9.tar.bz2) &gt; OUTPUT</code></p>
<p>So some benefits are already obvious.<br />
It is now much easier to find and download our patches.<br />
Downloading and seaching all of openSUSE is now much faster.<br />
And it works even on Thursdays (when our maintenance window often causes OBS downtimes).</p>
<p>It is meant to be a read-only mirror, so there is no point in opening pull-requests on github.</p>
<p>I hope, you enjoy it and have a lot of fun&#8230;</p>
]]></content:encoded>
			<wfw:commentRss>https://lizards.opensuse.org/2019/09/13/opensuse-obs-git-mirror/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>Tricks with IPFS</title>
		<link>https://lizards.opensuse.org/2019/08/07/tricks-with-ipfs/</link>
		<pubDate>Wed, 07 Aug 2019 19:45:06 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Infrastructure]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Server]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=13968</guid>
		<description><![CDATA[Since April I am using IPFS Now I wanted to document some neat tricks and details. When you have the hex-encoded sha256sum of a small file &#8211; for this example let&#8217;s use the GPLv3.txt on our media &#8211; sha256sum /ipns/opensuse.zq1.de/tumbleweed/repo/oss/GPLv3.txt 8ceb4b9ee5adedde47b31e975c1d90c73ad27b6b165a1dcd80c7c545eb65b90 Then you can use the hash to address content directly by prefixing it with [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>Since April I am  <a href="https://lizards.opensuse.org/2019/04/03/experimental-opensuse-mirror-via-ipfs/">using IPFS</a></p>
<p>Now I wanted to document some neat tricks and details.</p>
<p>When you have the hex-encoded sha256sum of a small file &#8211; for this example let&#8217;s use the GPLv3.txt on our media &#8211;<br />
<code>sha256sum /ipns/opensuse.zq1.de/tumbleweed/repo/oss/GPLv3.txt<br />
8ceb4b9ee5adedde47b31e975c1d90c73ad27b6b165a1dcd80c7c545eb65b90</code></p>
<p>Then you can use the hash to address content directly by prefixing it with /ipfs/f01551220 so it becomes</p>
<p>/ipfs/f015512208ceb4b9ee5adedde47b31e975c1d90c73ad27b6b165a1dcd80c7c545eb65b903</p>
<p>In theory this also works with SHA1 and the /ipfs/f01551114 prefix, but then you risk experiencing non-unique content like<br />
/ipfs/f0155111438762cf7f55934b34d179ae6a4c80cadccbb7f0a</p>
<p>And dont even think about using MD5.</p>
<p>For this trick to work, the file needs to be added with <code>ipfs add --raw-leaves</code> and it must be a single chunk &#8211; by default 256kB or smaller, but if you do the adding, you can also use larger chunks.</p>
<p>Here is a decoding of the different parts of the prefix:<br />
/ipfs/ is the common path for IPFS-addressed content<br />
f is the <a href="https://github.com/multiformats/multibase">multibase prefix</a> for hex-encoded data<br />
01 is for the <a href="https://docs.ipfs.io/guides/concepts/cid/#version-1">CID version 1</a><br />
55 is for <a href="https://github.com/multiformats/multicodec/blob/master/table.csv#L34">raw binary</a><br />
12 is for <a href="https://github.com/multiformats/multicodec/blob/master/table.csv#L6">sha2-256</a> (the default hash in IPFS)<br />
20 is for 32 byte = 256 bit length of hash</p>
<p>And finally, you can also access this content via the various IPFS-web-gateways:<br />
<a href="https://ipfs.io/ipfs/f015512208ceb4b9ee5adedde47b31e975c1d90c73ad27b6b165a1dcd80c7c545eb65b903">https://ipfs.io/ipfs/f015512208ceb4b9ee5adedde47b31e975c1d90c73ad27b6b165a1dcd80c7c545eb65b903</a></p>
<p>You can also do the same trick with other multibase encodings of the same data &#8211; e.g. <a href="https://ipfs.io/ipfs/010101010100010010001000001000110011101011010010111001111011100101101011011110110111011110010001111011001100011110100101110101110000011101100100001100011100111010110100100111101101101011000101100101101000011101110011011000000011000111110001010100010111101011011001011011100100000011">base2</a></p>
<p>Base2 looks pretty geeky, but so far I have not found practical applications.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Debugging jenkins</title>
		<link>https://lizards.opensuse.org/2019/07/31/debugging-jenkins/</link>
		<comments>https://lizards.opensuse.org/2019/07/31/debugging-jenkins/#comments</comments>
		<pubDate>Wed, 31 Jul 2019 08:04:23 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Documentation]]></category>
		<category><![CDATA[Infrastructure]]></category>
		<category><![CDATA[Quality Assurance]]></category>
		<category><![CDATA[CI]]></category>
		<category><![CDATA[jenkins]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=13945</guid>
		<description><![CDATA[We had strange near-daily outages of our internal busy jenkins for some weeks. To get to the root cause of the issue, we enabled remote debugging with -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9010 -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=ci.suse.de -Dcom.sun.management.jmxremote.password.file=/var/lib/jenkins/jmxremote.password and attached visualvm to see what it was doing. This showed the number of threads and memory usage in a sawtooth pattern. Every [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>We had strange near-daily outages of our internal busy jenkins for some weeks.</p>
<p>To get to the root cause of the issue, we enabled remote debugging with</p>
<blockquote><p>
-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9010 -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=ci.suse.de -Dcom.sun.management.jmxremote.password.file=/var/lib/jenkins/jmxremote.password</p></blockquote>
<p>and attached visualvm to see what it was doing.<br />
This showed the number of threads and memory usage in a sawtooth pattern. Every time the garbage collector ran, it dropped 500-1000 threads.</p>
<p>Today we noticed that every time it threw these <code>java.lang.OutOfMemoryError: unable to create new native thread</code> errors, the maximum number of threads was 2018&#8230; suspiciously close to 2048. Looking for the same time in journalctl showed<br />
<code>kernel: cgroup: fork rejected by pids controller in /system.slice/jenkins.service</code></p>
<p>So it was systemd refusing java&#8217;s request for a new thread and jenkins not handling that gracefully in all cases.<br />
That was easily avoided with a<br />
<code>TasksMax=8192</code></p>
<p>Now the new peak was at 4890 live threads and jenkins served all Geekos happily ever after.</p>
]]></content:encoded>
			<wfw:commentRss>https://lizards.opensuse.org/2019/07/31/debugging-jenkins/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>experimental openSUSE mirror via IPFS</title>
		<link>https://lizards.opensuse.org/2019/04/03/experimental-opensuse-mirror-via-ipfs/</link>
		<comments>https://lizards.opensuse.org/2019/04/03/experimental-opensuse-mirror-via-ipfs/#comments</comments>
		<pubDate>Wed, 03 Apr 2019 13:41:39 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Factory]]></category>
		<category><![CDATA[Infrastructure]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Toolchain]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=13716</guid>
		<description><![CDATA[The InterPlanetary File System (IPFS) can be used to provide files in a more efficient and distributed way than HTTP. Our filesystem repo already has the go-ipfs client. You use it with ipfs daemon --init And then you can add my Tumbleweed mirror with zypper ar http://127.0.0.1:8080/ipns/opensuse.zq1.de./tumbleweed/repo/oss/ ipfs-oss You can also browse the content online [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/InterPlanetary_File_System">InterPlanetary File System</a> (IPFS) can be used to provide files in a more efficient and distributed way than HTTP.</p>
<p>Our <a href="https://software.opensuse.org/package/go-ipfs">filesystem repo</a> already has the go-ipfs client.</p>
<p>You use it with<br />
<code>ipfs daemon --init</code></p>
<p>And then you can add my Tumbleweed mirror with<br />
<code>zypper ar http://127.0.0.1:8080/ipns/opensuse.zq1.de./tumbleweed/repo/oss/ ipfs-oss</code></p>
<p>You can also browse the content online at<br />
<a href="http://opensuse.zq1.de./tumbleweed/repo/oss/">http://opensuse.zq1.de./tumbleweed/repo/oss/</a> . During my testing I found that the results are sometimes inappropriately cached on the Cloudflare CDN, so if you used it under this URL without the ipfs client, this might throw signature errors in zypper.</p>
<p>On the server side, the mirror is updated using the <code>syncopensuse</code> script from<br />
<a href="https://github.com/bmwiedemann/opensusearchive">https://github.com/bmwiedemann/opensusearchive</a> and consistency of the repo is verified with <code>checkrepo</code></p>
<p>When a complete repo was synced, <a href="https://github.com/bmwiedemann/dynaname">dynaname</a> updates a DNS entry to point to the new head:</p>
<blockquote><p>
&gt; host -t txt _dnslink.opensuse.zq1.de.<br />
_dnslink.opensuse.zq1.de is an alias for tumbleweedipfs.d.zq1.de.<br />
tumbleweedipfs.d.zq1.de descriptive text &#8220;Last update: 2019-04-03 12:23:43 UTC&#8221;<br />
tumbleweedipfs.d.zq1.de descriptive text &#8220;dnslink=/ipfs/QmSXEVuU5z23rDxMyFYDhSAUaGRUPswuSXD3aVsBEzucjE&#8221;</p></blockquote>
<p>If you got spare bandwidth and 300 GB disk on some public server, you could also host a mirror of today&#8217;s version, simply by doing <code>ipfs pin add QmSXEVuU5z23rDxMyFYDhSAUaGRUPswuSXD3aVsBEzucjE</code></p>
<p>This is a permalink: <a href="http://127.0.0.1:8080/ipfs/QmSXEVuU5z23rDxMyFYDhSAUaGRUPswuSXD3aVsBEzucjE">http://127.0.0.1:8080/ipfs/QmSXEVuU5z23rDxMyFYDhSAUaGRUPswuSXD3aVsBEzucjE</a> also browsable via <a href="https://cloudflare-ipfs.com/ipfs/QmSXEVuU5z23rDxMyFYDhSAUaGRUPswuSXD3aVsBEzucjE">any public IPFS gateway</a>. This means, it will always remain on the 20190401 version of Tumbleweed and no changes in content are possible &#8211; similar to how a git commit ID always refers to the same data.</p>
<p>So why did I create this IPFS mirror? That is related to my work on <a href="https://en.opensuse.org/openSUSE:Reproducible_Builds">reproducible builds for openSUSE</a>. There it regularly happened that published Tumbleweed binaries were built with libraries, compilers and toolchains that were no longer available in current Tumbleweed. This prevented me from verifying that the published binaries were indeed built correctly without manipulation on the OBS build workers.</p>
<p>Now, with this archive of rpms easily available, it was possible to verify many more Tumbleweed packages than before. And most importantly, it remains possible to independently verify even after Tumbleweed moves on to newer versions. This data is going to stay available as long as anyone pins it on a reachable server. I&#8217;m going to pin it as long as it remains relevant to me, so probably a bit until after the next full Tumbleweed rebuild &#8211; maybe 6 to 12 months.</p>
<p>Thus, it now is even less easy to sneak in binary backdoors during our package build process.</p>
]]></content:encoded>
			<wfw:commentRss>https://lizards.opensuse.org/2019/04/03/experimental-opensuse-mirror-via-ipfs/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		</item>
		<item>
		<title>Report from the reproducible builds summit 2018</title>
		<link>https://lizards.opensuse.org/2018/12/17/report-from-the-reproducible-builds-summit-2018/</link>
		<comments>https://lizards.opensuse.org/2018/12/17/report-from-the-reproducible-builds-summit-2018/#comments</comments>
		<pubDate>Mon, 17 Dec 2018 10:36:10 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Factory]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Toolchain]]></category>
		<category><![CDATA[Paris]]></category>
		<category><![CDATA[reproducible-builds]]></category>
		<category><![CDATA[Summit]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=13522</guid>
		<description><![CDATA[Last week I attended the reproducible builds world summit in Paris. It was very well organized by Holger, Gunner and their hidden helpers in the background. Very similar to the last 2 summits I attended in Berlin. Because we were around 50 participants, introductions and announcements were the only things done in the big group. [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>Last week I attended <a href="https://reproducible-builds.org/events/paris2018/">the reproducible builds world summit in Paris</a>.<br />
It was very well organized by Holger, Gunner and their hidden helpers in the background. Very similar to the last 2 summits I attended in Berlin.</p>
<p>Because we were around 50 participants, introductions and announcements were the only things done in the big group. All actual work happened in 5-10 smaller circles.</p>
<p>We had participants from large companies like Google (with bazel), MicroSoft and Huawei, but also from many distributions and open source projects. Even MirageOS as non-Linux OS.</p>
<p>We did knowledge-sharing, refine definitions of terms, evolve concepts like &#8220;rebuilders&#8221; for verifying builds and allow users to better trust software they install, and such.</p>
<p>I learned about <a href="https://tests.reproducible-builds.org/reproducible.sql.xz">the undocumented DB dump</a> (153 MB) and  <a href="https://tests.reproducible-builds.org/reproducibledb.html">DB schema</a></p>
<p>And we had some hacking time, too, so there is now<br />
<a href="https://jenkins.debian.net/job/reproducible_opensuse_import_json/">a jenkins job</a> that renders <a href="https://tests.reproducible-builds.org/opensuse/factory/x86_64/index_FTBR.html">the list of unreproducible openSUSE Factory packages</a>.</p>
<p>Also, <a href="https://maintainer.zq1.de/?pkg=bash">my maintainer tool</a> now has added support for the Alpine Linux distribution, thanks to help by one of its maintainers: Natanael Copa.<br />
This is meant to help all cross-distro collaboration, not just for reproducible builds.</p>
<p>There is still work to be done to make better use of Mitre CPE to map package names across distributions.</p>
<p>I think, one major benefit of the summit was all the networking and talking going on, so that we have an easier time working with each other over the internet in the future.</p>
]]></content:encoded>
			<wfw:commentRss>https://lizards.opensuse.org/2018/12/17/report-from-the-reproducible-builds-summit-2018/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>The issues with contributing to projects only once</title>
		<link>https://lizards.opensuse.org/2017/06/04/the-issues-with-contributing-to-projects-only-once/</link>
		<comments>https://lizards.opensuse.org/2017/06/04/the-issues-with-contributing-to-projects-only-once/#comments</comments>
		<pubDate>Sun, 04 Jun 2017 04:06:31 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Build Service]]></category>
		<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Packaging]]></category>
		<category><![CDATA[Programming]]></category>
		<category><![CDATA[contributing]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=12523</guid>
		<description><![CDATA[I work to improve the openSUSE Tumbleweed (GNU/)Linux distribution. Specifically I make sure that all packages can be built twice on different hosts and still produce identical results, which has multiple benefits. This generates a lot of patches in a single week. OBS Sometimes it is enough to adjust the .spec file &#8211; that is [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>I work to improve the openSUSE Tumbleweed (GNU/)Linux distribution. Specifically I make sure that all packages can be built twice on different hosts and still produce identical results, which has <a href="https://reproducible-builds.org/docs/buy-in/">multiple benefits</a>. This generates <a href="https://reproducible.alioth.debian.org/blog/posts/109/">a lot of patches</a> in a single week.</p>
<p><strong>OBS</strong><br />
Sometimes it is enough to adjust the .spec file &#8211; that is a small text file usually specific to us. Then <a href="https://news.opensuse.org/2011/09/27/get-your-package-in-factory-for-12-1/">it is straight-forward</a></p>
<ol>
<li>osc bco</li>
<li>cd $PROJECT/$PACKAGE</li>
<li>optional: spec_add_patch $MY.patch $SOME.spec</li>
<li>edit *.spec</li>
<li>osc build</li>
<li>osc vc</li>
<li>osc ci</li>
<li>osc sr</li>
</ol>
<p>And OBS will even auto-clean the branch when the submit-request is accepted. And it has a &#8216;tasks&#8217; page to see and track SRs in various stages. For the spec_add_patch to work, you need to do once<br />
ln -s /usr/lib/build/spec_add_patch /usr/local/bin/</p>
<p>When you want to contribute patches upstream, so that other distributions benefit from your improvements as well, then you first need to find out, where they collaborate. A good starting point is the URL field in the .spec file, but a google search for &#8216;contribute $PROJECT&#8217; often is better.</p>
<p><strong>github</strong><br />
Then there are those many projects hosted on github, where it is also pretty low effort, because I already have the account and it even remains signed in. But some repos on github are only read-only mirrors.</p>
<ol>
<li>check pull requests, if some have been merged recently</li>
<li>fork the project</li>
<li>git clone git@github.com:&#8230;</li>
<li>cd $REPO</li>
<li>edit $FILES</li>
<li>git commit -a</li>
<li>git push</li>
<li>open pull request</li>
<li>maybe have to sign a CLA for the project</li>
<li>When change is accepted, delete fork to not clutter up repository list too much (on github under settings)</li>
</ol>
<p><strong>sourceforge</strong><br />
The older brother of github. They integrate various ways of contributing. The easiest one is to open a Ticket (Patch or Bug) and attach the .patch you want them to merge with a good description. While many developers do not have the time and energy to debug every bug you file, applying patches is much easier, so gets your issue fixed with a higher chance.</p>
<p><strong>devel Mailinglist</strong><br />
Some projects collaborate mainly through their development MLs, then I need to</p>
<ol>
<li>subscribe</li>
<li>confirm the subscription</li>
<li>git format-patch origin/master</li>
<li>git send-email &#8211;to $FOO-devel@&#8230; &#8211;from $MYSUBSCRIBEDEMAIL 000*.patch</li>
<li>wait for replies</li>
<li>if it is a high-volume ML, also add an IMAP folder and an entry to .procmailrc</li>
<li>unsubscribe</li>
<li>confirm</li>
</ol>
<p><strong>project bugtracker</strong><br />
Like https://bugzilla.gnome.org/ https://bugs.python.org/ https://bugs.ruby-lang.org/ https://bz.apache.org/bugzilla/</p>
<ol>
<li>create unique email addr</li>
<li>sign up for account</li>
<li>add info to my account list</li>
<li>optional: search for existing bug (90% of the time there is none)</li>
<li>file bug</li>
<li>attach patch</li>
</ol>
<p>So as you can see there is a wide range of ways. And most of them have some initial effort that you would only have once&#8230; But then I only contribute once per project, so I always pay that.</p>
<p>Thus, please make it easy for people to contribute one simple fix.</p>
]]></content:encoded>
			<wfw:commentRss>https://lizards.opensuse.org/2017/06/04/the-issues-with-contributing-to-projects-only-once/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>How we run our OpenStack cloud</title>
		<link>https://lizards.opensuse.org/2017/01/23/how-we-run-our-openstack-cloud/</link>
		<pubDate>Mon, 23 Jan 2017 13:12:10 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Documentation]]></category>
		<category><![CDATA[Miscellaneous]]></category>
		<category><![CDATA[cloud]]></category>
		<category><![CDATA[openstack]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=12244</guid>
		<description><![CDATA[This post it to document how we setup cloud.suse.de which is one of our many internal SUSE OpenStack Cloud deployments for use by R&#38;D. In 2016-06 we started the deployment with SOC6 on 4 nodes. 1 controller and 3 compute nodes that also served for ceph (distributed storage) with their 2nd HDD. Since the nodes [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>This post it to document how we setup cloud.suse.de which is one of our many internal SUSE OpenStack Cloud deployments for use by R&amp;D.</p>
<p>In 2016-06 we started the deployment with SOC6 on 4 nodes. 1 controller and 3 compute nodes that also served for ceph (distributed storage) with their 2nd HDD. Since the nodes are from 2012 they only have 1gbit network and spinning disks. Thus ceph only delivers ~50 MB/s which is sufficient for many use cases.</p>
<p>We did not deploy that cloud with HA, even though our product supports it. The two main reasons for that are</p>
<ul>
<li>that it will use up two or three nodes instead of one for controller services, which is significant if you start out with only 4 (and grow to 6)</li>
<li>that it increases the complexity of setup, operations and debugging and thus might lead to decreased availability of the cloud service</li>
</ul>
<p>Then we have a limited supply of vlans even though technically they are just numbers between 1 and 4095, in SUSE we do allocations to be able to switch together networks from further away. So we could not use vlan mode in neutron if we want to allow software defined network (=SDN) (we did not in old.cloud.suse.de and I did not hear complaints, but now I see a lot of people using SDN)<br />
So we went with ovs+vxlan +dvr (open vSwitch + Virtual eXtensible LAN + Distributed Virtual Router) because that allows VMs to remain reachable even when the controller node reboots.<br />
But then I found that they cannot use DNS during that time, because distributed virtual DNS was not yet implemented. And ovs has some <a href="https://bugzilla.suse.com/show_bug.cgi?id=998363">annoying bugs</a> are hard to debug and fix. So I built ugly workarounds that mostly hide^Wsolve the problems from our users&#8217; point of view.<br />
For the next cloud deployment, I will try to use linuxbridge+vlan or linuxbridge+vxlan mode.<br />
And the uptime is pretty good. But it could be better with proper monitoring.</p>
<p>Because we needed to redeploy multiple times before we got all the details right and to document the setup, we scripted most of the deployment with qa_crowbarsetup (which is part of our CI) and extra files in <a href="https://github.com/SUSE-Cloud/automation/tree/production/scripts/productioncloud">https://github.com/SUSE-Cloud/automation/tree/production/scripts/productioncloud</a>. The only part not in there are the passwords.</p>
<p>We use proper SSL certs from our internal SUSE CA.<br />
For that we needed to install that root CA on all involved nodes.</p>
<p>We use kvm, because it is the most advanced and stable of the supported hypervisors. Xen might be a possible 2nd choice. We use two custom kvm patches to fix nested virt on our G3 Opteron CPUs.</p>
<p>Overall we use 3 vlans. One each for admin, public/floating, sdn/storage networks.<br />
We increased the default /24 IP ranges because we needed more IPs in the fixed and public/floating networks</p>
<p>For authentication, we use our internal R&amp;D LDAP server, but since it does not have information about user&#8217;s groups, I wrote a perl script to pull that information from the Novell/innerweb LDAP server and export it as json for use by the <a href="https://github.com/SUSE-Cloud/keystone-hybrid-backend/blob/liberty/hybrid_json_assignment.py">hybrid_json</a> assignment backend I wrote.</p>
<p>In addition I wrote a <a href="https://github.com/SUSE-Cloud/automation/blob/master/scripts/cloud-stats.sh">cloud-stats.sh</a> to email weekly reports about utilization of the cloud and another script to tell users about which instances they still have, but might have forgotten.</p>
<p>On the cloud user side, we and other people use one or more of</p>
<ul>
<li>Salt-cloud</li>
<li>Nova boot</li>
<li>salt-ssh</li>
<li>terraform</li>
<li>heat</li>
</ul>
<p>to script instance setup and administration.</p>
<p>Overall we are now hosting 70 instance VMs on 5 compute nodes that together have cost us below 20000€</p>
]]></content:encoded>
			</item>
		<item>
		<title>How to build OS images without kiwi</title>
		<link>https://lizards.opensuse.org/2016/12/26/how-to-build-os-images-without-kiwi/</link>
		<pubDate>Mon, 26 Dec 2016 05:56:48 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Toolchain]]></category>
		<category><![CDATA[altimagebuild]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=12224</guid>
		<description><![CDATA[kiwi has long been the one standard way of building images in openSUSE, but even though there exist extensive writings on how to use it, for many it is still an arcane thing better left to the Great Magicians. Thus, I started to use a simpler alternative image building method, named altimagebuild when I built [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>kiwi has long been the one standard way of building images in openSUSE, but even though there exist extensive writings on how to use it, for many it is still an arcane thing better left to the Great Magicians.</p>
<p>Thus, I started to use a simpler alternative image building method, named altimagebuild when I built our first working <a href="https://lizards.opensuse.org/2013/09/07/new-raspberry-pi-image/">Raspberry Pi images</a> in 2013 and now I re-used that to build x86_64 VM images at<br />
<a href="https://build.opensuse.org/package/show/home:bmwiedemann/altimagebuild">https://build.opensuse.org/package/show/home:bmwiedemann/altimagebuild</a><br />
after I found out that it even works in OBS, including publishing the result to our mirror infrastructure.<br />
It is still in rpm format because of how it is produced, so you have to use unrpm to get to the image file.</p>
<p>This method uses 3 parts.</p>
<ul>
<li>a .spec file that lists packages to be pulled into the image</li>
<li>a mkrootfs.sh that converts the build system into the future root filesystem you want</li>
<li>a mkimage.sh that converts the rootfs into a filesystem image</li>
</ul>
<p>The good thing about it is that you do not need specialized external tools, because everything is hard-coded in the scripts.<br />
And the bad thing about it is that everything is hard-coded in the scripts, so it is hard to share general improvements over a wider range of images.</p>
<p>In the current version, it builds cloud-enabled partitionless images (which is nice for VMs because you can just use resize2fs to get a larger filesystem and if you later want to access your VM&#8217;s data from outside, you simply use mount -o loop)<br />
But it can build anything you want.</p>
<p>To make your own build, do <code>osc checkout home:bmwiedemann/altimagebuild &amp;&amp; cd $_ &amp;&amp; osc build openSUSE_Leap_42.2</code></p>
<p>So what images would you want to build?</p>
]]></content:encoded>
			</item>
		<item>
		<title>targitter project &#8211; about OBS, tars and git</title>
		<link>https://lizards.opensuse.org/2015/10/05/targitter-project-about-obs-tars-and-git/</link>
		<pubDate>Mon, 05 Oct 2015 19:24:57 +0000</pubDate>
		<dc:creator><![CDATA[bmwiedemann]]></dc:creator>
				<category><![CDATA[Build Service]]></category>
		<category><![CDATA[Distribution]]></category>
		<category><![CDATA[Packaging]]></category>
		<category><![CDATA[git]]></category>
		<category><![CDATA[obs]]></category>
		<category><![CDATA[tar]]></category>
		<category><![CDATA[targitter]]></category>

		<guid isPermaLink="false">http://lizards.opensuse.org/?p=11543</guid>
		<description><![CDATA[In OBS we use source tarballs everywhere to build rpms (and debs) from. This has at least two major downsides: Storing all old tar files takes up a lot of disk space OBS workflows with .tar files and patches are rather different and somewhat disconnected from the git workflows we usually use everywhere else these [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>In OBS we use source tarballs everywhere to build rpms (and debs) from.</p>
<p>This has at least two major downsides:</p>
<ol>
<li>
Storing all old tar files takes up a lot of disk space
</li>
<li>
OBS workflows with .tar files and patches are rather different and somewhat disconnected from the git workflows we usually use everywhere else these days. E.g. for the SUSE OpenStack Cloud team we have a &#8220;trackupstream&#8221; jenkins job, that pulls the latest git version into a tarball once every day.
</li>
</ol>
<p>Fedora already keeps <a href="http://pkgs.fedoraproject.org/cgit/perl-OpenGL.git/plain/">their metadata</a> in git, but only a hash of the tarball.</p>
<p>So as one first step, I used two rather different projects to see how different the space usage would be. On the slow side I used 20 gtk2 tarballs from the last 5 years and on the fast side, I used 31 openstack-nova tarballs from Cloud:OpenStack:Master project from the last 5 months.<br />
I used <a href="https://www.zq1.de/~bernhard/targitter/eval/">scripts</a> that uncompressed each tarball, added it to a git repo and used git gc to trigger git&#8217;s compression. </p>
<p>Here are the resulting cumulative size graphs:</p>
<p><img src="https://www.zq1.de/~bernhard/targitter/eval/size-gtk2.svg" alt="gtk2 size graph" /><br />
<img src="https://www.zq1.de/~bernhard/targitter/eval/size-nova.svg" alt="nova size graph" /></p>
<p>The raw numbers after 20 tarballs: for nova the ratio is 89772:7344 = 12.2 and for gtk2 the ratio is 296836:53076 = 5.6</p>
<p>What do you think: would it be worth the effort to use more git in our OBS workflows?</p>
<p>Do we care about being able to reproduce the original tarballs? While this is possible, it has some challenges in differing file-ordering, timestamps, file-ownerships and <a href="https://github.com/jmacd/xdelta/pull/207">compression levels</a>.<br />
Or would it be enough if OBS converted a tarball into a signed commit (so it cannot be forged without people being able to notice)?</p>
<p>Do you know of a tool that can uncompress tarballs in a way that allows to track the content as single files, and allows to later re-create the original verbatim tarball, such that upstream signatures would still match?</p>
]]></content:encoded>
			</item>
	</channel>
</rss>
